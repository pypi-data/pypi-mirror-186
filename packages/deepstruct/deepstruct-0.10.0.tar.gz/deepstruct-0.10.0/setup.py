# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['deepstruct']

package_data = \
{'': ['*']}

install_requires = \
['deprecated>=1.2.10,<2.0.0',
 'importlib-metadata>=4.4,<5.0',
 'importlib-resources>=5.0,<6.0',
 'networkx>=2.0,<3.0',
 'numpy>=1.21,<2.0',
 'semantic_version>=2.10,<3.0',
 'torch>=1.7,<2.0']

setup_kwargs = {
    'name': 'deepstruct',
    'version': '0.10.0',
    'description': '',
    'long_description': '# deepstruct - neural network structure tool [![PyPI version](https://badge.fury.io/py/deepstruct.svg)](https://badge.fury.io/py/deepstruct) ![Tests](https://github.com/innvariant/deepstruct/workflows/Tests/badge.svg) [![Documentation Status](https://readthedocs.org/projects/deepstruct/badge/?version=latest)](https://deepstruct.readthedocs.io/en/latest/?badge=latest) [![Downloads](https://pepy.tech/badge/deepstruct)](https://pepy.tech/project/deepstruct)  [![Python 3.8](https://img.shields.io/badge/python-3.8-blue.svg)](https://www.python.org/downloads/release/python-380/)\nCreate deep neural networks based on very different kinds of graphs or use *deepstruct* to extract the structure of your deep neural network.\n\nDeepstruct combines tools for fusing machine learning and graph theory.\nWe are fascinated with the interplay of end-to-end learnable, locally restricted models and their graph theoretical properties.\nSearching for evidence of the structural prior hypothesis.\nInterested in pruning, neural architecture search or learning theory in general?\n\nSee [examples](#examples) below or [read the docs](https://deepstruct.readthedocs.io).\n\nWe\'re glad if you reference our work\n```bibtex\n@article{stier2022deepstruct,\n  title={deepstruct -- linking deep learning and graph theory},\n  author={Stier, Julian and Granitzer, Michael},\n  journal={Software Impacts},\n  volume={11},\n  year={2022},\n  publisher={Elsevier}\n}\n```\n\n## Installation\n- With **pip** from PyPi: ``pip install deepstruct``\n- With **conda** in your *environment.yml* (recommended for reproducible experiments):\n```yaml\nname: exp01\nchannels:\n- defaults\ndependencies:\n- pip>=20\n- pip:\n    - deepstruct\n```\n- With **poetry** (recommended for *projects*) using PyPi: ``poetry add deepstruct``\n- From public GitHub: ``pip install --upgrade git+ssh://git@github.com:innvariant/deepstruct.git``\n\n## Quick usage: multi-layered feed-forward neural network on MNIST\nThe simplest implementation is one which provides multiple layers with binary masks for each weight matrix.\nIt doesn\'t consider any skip-layer connections.\nEach layer is then connected to only the following one.\n```python\nimport deepstruct.sparse\n\nmnist_model = deepstruct.sparse.MaskedDeepFFN((1, 28, 28), 10, [100]*10, use_layer_norm=True)\n```\nThis is a ready-to-use pytorch module which has ten layers of each one hundred neurons and applies layer normalization before each activation.\nTraining it on any dataset will work out of the box like every other pytorch module.\nHave a look on [pytorch ignite](https://pytorch.org/ignite/) or [pytorch lightning](https://github.com/Lightning-AI/lightning/) for designing your training loops.\nYou can set masks on the model via\n```python\nimport deepstruct.sparse\nfor layer in deepstruct.sparse.maskable_layers(mnist_model):\n    layer.mask[:, :] = True\n```\nand if you disable some of these mask elements you have defined your first sparse model.\n\n\n## Examples\nSpecify structures by prior design, e.g. random social networks transformed into directed acyclic graphs:\n```python\nimport networkx as nx\nimport deepstruct.sparse\n\n# Use networkx to generate a random graph based on the Watts-Strogatz model\nrandom_graph = nx.newman_watts_strogatz_graph(100, 4, 0.5)\nstructure = deepstruct.graph.CachedLayeredGraph()\nstructure.add_edges_from(random_graph.edges)\nstructure.add_nodes_from(random_graph.nodes)\n\n# Build a neural network classifier with 784 input and 10 output neurons and the given structure\nmodel = deepstruct.sparse.MaskedDeepDAN(784, 10, structure)\nmodel.apply_mask()  # Apply the mask on the weights (hard, not undoable)\nmodel.recompute_mask()  # Use weight magnitude to recompute the mask from the network\npruned_structure = model.generate_structure()  # Get the structure -- a networkx graph -- based on the current mask\n\nnew_model = deepstruct.sparse.MaskedDeepDAN(784, 10, pruned_structure)\n```\n\nDefine a feed-forward neural network (with no skip-layer connections) and obtain its structure as a graph:\n```python\nimport deepstruct.sparse\n\nmodel = deepstruct.sparse.MaskedDeepFFN(784, 10, [100, 100])\n# .. train model\nmodel.generate_structure()  # a networkx graph\n```\n\n\n### Recurrent Neural Networks with sparsity\n```python\nimport torch\nimport deepstruct.recurrent\nimport numpy as np\n\n# A sequence of size 15 with one-dimensional elements which could e.g. be labelled\n# BatchSize x [(1,), (2,), (3,), (4,), (5,), (0,), (0,), (0,)] --> [ label1, label2, ..]\nbatch_size = 100\nseq_size = 15\ninput_size = 1\nmodel = deepstruct.recurrent.MaskedDeepRNN(\n    input_size,\n    hidden_layers=[100, 100, 1],\n    batch_first=True,\n    build_recurrent_layer=deepstruct.recurrent.MaskedLSTMLayer,\n)\nrandom_input = torch.tensor(\n    np.random.random((batch_size, seq_size, input_size)),\n    dtype=torch.float32,\n    requires_grad=False,\n)\nmodel.forward(random_input)\n```\n\n\n\n## Sparse Neural Network implementations\n![Sparse Network Connectivity on zeroth order with a masked deep feed-forward neural network](docs/masked-deep-ffn.png)\n![Sparse Network Connectivity on zeroth order with a masked deep neural network with skip-layer connections](docs/masked-deep-dan.png)\n![Sparse Network Connectivity on second order with a masked deep cell-based neural network](docs/masked-deep-cell-dan.png)\n\n**What\'s contained in deepstruct?**\n- ready-to-use models in pytorch for learning instances on common (supervised/unsupervised) datasets from which a structural analysis is possible\n- model-to-graph transformations for studying models from a graph-theoretic perspective\n\n**Models:**\n- *deepstruct.sparse.MaskableModule*: pytorch modules that contain explicit masks to enforce (mostly zero-ordered) structure\n- *deepstruct.sparse.MaskedLinearLayer*: pytorch module with a simple linear layer extended with masking capability.\nSuitable if you want to have linear-layers on which to enforce masks which could be obtained through pruning, regularization or other other search techniques.\n- *deepstruct.sparse.MaskedDeepFFN*: feed-forward neural network with any width and depth and easy-to-use masks.\nSuitable for simple and canonical pruning research on zero-ordered structure\n- *deepstruct.sparse.MaskedDeepDAN*: feed-forward neural network with skip-layer connections based on any directed acyclic network.\nSuitable for arbitrary structures on zero-order and on that level most flexible but also computationally expensive.\n- *deepstruct.sparse.DeepCellDAN*: complex module based on a directed acyclic network and custom cells on third-order structures.\nSuitable for large-scale neural architecture search\n- *deepstruct.recurrent.MaskedDeepRNN*: multi-layered network with recurrent layers which can be masked\n\n## What is the orders of structure?\n- zero-th order: weight-level\n- first order: kernel-level (filter, channel, blocks, cells)\n- second order: layers\n\nThere is various evidence across empirical machine learning studies that the way artificial neural networks are structurally connected has a (minor?) influence on performance metrics such as the accuracy or probably even on more complex concepts such as adversarial robustness.\nWhat do we mean by "structure"?\nWe define structure over graph theoretic properties given a computational graph with very restricted non-linearities.\nThis includes all major neural network definitions and lets us study them from the perspective of their *representation* and their *structure*.\nIn a probabilistic sense, one can interprete structure as a prior to the model and despite single-layered wide networks are universal function approximators we follow the hypothesis that given certain structural priors we can find models with better properties.\n\nBefore considering implementations, one should have a look on possible representations of Sparse Neural Networks.\nIn case of feed-forward neural networks (FFNs) the network can be represented as a list of weight matrices.\nEach weight matrix represents the connections from one layer to the next.\nHaving a network without some connections then means setting entries in those matrices to zero.\nRemoving a particular neuron means setting all entries representing its incoming connections to zero.\n\nHowever, sparsity can be employed on various levels of a general artificial neural network.\nZero order sparsity would remove single weights (representing connections) from the network.\nFirst order sparsity removes groups of weights within one dimension of a matrix from the network.\nSparsity can be employed on connection-, weight-, block-, channel-, cell-level and so on.\nImplementations respecting the areas for sparsification can have drastical differences.\nThus there are various ways for implementing Sparse Neural Networks.\n\n\n# Artificial PyTorch Datasets\n![A custom artificial landscape Stier2020B for testing function approximation](docs/artificial-landscape-approximation.png)\nWe provide some simple utilities for artificial function approximation.\nLike polynomials, neural networks are universal function approximators on bounded intervals of compact spaces.\nTo test, you can easily define a function of any finite dimension, e.g. $f: \\mathbb{R}^2\\rightarrow\\mathbb{R}, (x,y)\\mapsto 20 + x - 1.8*(y-5) + 3 * np.sin(x + 2 * y) * y + (x / 4) ** 4 + (y / 4) ** 4$:\n```python\nimport numpy as np\nimport torch.utils.data\nfrom deepstruct.dataset import FuncDataset\nfrom deepstruct.sparse import MaskedDeepFFN\n\n# Our artificial landscape: f: R^2 -> R\n# Have a look at https://github.com/innvariant/eddy for some visual examples\n# You could easily define arbitrary functions from R^a to R^b\nstier2020B1d = lambda x, y: 20 + x - 1.8*(y-5) + 3 * np.sin(x + 2 * y) * y + (x / 4) ** 4 + (y / 4) ** 4\nds_input_shape = (2,)  # specify the number of input dimensions (usually a one-sized tensor if no further structures are used)\n# Explicitly define the target function for the dataset which returns a numpy array of our above function\n# By above definition x is two-dimensional, so you have access to x[0] and x[1]\nfn_target = lambda x: np.array([stier2020B1d(x[0], x[1])])\n# Define a sampling strategy for the dataset, e.g. uniform sampling the space\nfn_sampler = lambda: np.random.uniform(-2, 2, size=ds_input_shape)\n# Define the dataset given the target function and your sampling strategy\n# This simply wraps your function into a pytorch dataset and provides you with discrete observations\n# Your model will later only know those observations to come up with an approximate solution of your target\nds_train = FuncDataset(fn_target, shape_input=ds_input_shape, size=500)\n\n# Calculate the output shape given our target function .. usually simply a (1,)-dimensional output\nds_output_shape = fn_target(fn_sampler()).shape\n\n# As usual in pytorch, you can simply wrap your dataset with a loading strategy ..\n# This ensures e.g. that you do not iterate over your observations in the exact same manner\n# In case you sample first 100 examples of a binary classification dataset with label 1 and then another\n# 100 with label 2 it might impact your training .. so this ensures you have an e.g. random sampling strategy over the dataset\nbatch_size = 100\ntrain_sampler = torch.utils.data.SubsetRandomSampler(np.arange(len(ds_train), dtype=np.int64))\ntrain_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n\n# Define a model for which we can later extract its structure or impose sparsity constraints\nmodel = MaskedDeepFFN(2, 1, [50, 20])\n\n# Iterate over your training set\nfor feat, target in train_loader:\n    print(feat, target)\n\n    # feed it into a model to learn\n    prediction = model.forward(feat)\n\n    # compute a loss based on the expected target and the models prediction\n    # ..\n```\n',
    'author': 'Julian Stier',
    'author_email': 'julian.stier@uni-passau.de',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/innvariant/deepstruct',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.8,<4.0',
}


setup(**setup_kwargs)
