#######################################################################
#                             Data Paths                              #
#######################################################################
data_paths:
  metadata:
    runlib2samp_file: "rawdata/rl2s.tsv"
    setfile_glob: "rawdata/samplesets/*.txt"
  references:
    lambda:
      fasta: "rawdata/reference/genome.fa"
  kraken:
    Viral: "rawdata/kraken/Viral"
  kaiju:
    Viral:
      nodes: "rawdata/kaiju/Viral/nodes.dmp"
      fmi: "rawdata/kaiju/Viral/kaiju_db_viruses.fmi"
  centrifuge:
    lambda: "rawdata/centrifuge/lambda/lambda.1.cf"
  temp_prefix: "tmp/"
  persistent_prefix: "output/"

#######################################################################
#                      Sample Set Configuration                       #
#######################################################################
#
# This section is where we tell snakemake which files to generate for each set of samples.
# Samplesets are configured as files of sample names (see setfile_glob above). 

samplesets:
  all_samples:
    megahit:
      aligners:
        - bwa
      references:
        - lambda
    align:
      aligners:
        - bwa
        - ngm
      references:
        - lambda
      unmapped_reads: true
      stats: true
      qualimap: true
    kraken:
     # Output (un)-classified read fastqs from Kraken?
      reads: false
      dbs:
        - Viral
    kaiju:
      dbs:
        - Viral
    centrifuge:
      dbs: []
        # Centrifuge is disabled in this example as we use this in CI testing
        # and centrifuge uses too much RAM, causing our tests to fail. To run
        # centrifuge, supply an array/list of databases here as one does above
        # for Kraken & Kaiju, for exampe:
        #
        # - lambda
    persample_reads: false # don't specifically make sample.fastq.gz
    fastqc: true
    mash: true
    kwip: true
    varcall:
      ploidy: 2
      theta_prior: 0.01
      callers:
        - mpileup
        - freebayes
      aligners:
        - bwa
      refs:
        - lambda
      filters:
        - default
      snpeff: false


tool_settings:
  # sets a trade-off between compression time and disk usage. If disk is
  # limiting, increase, if CPUS expensive, decrease. Default should be 6.
  ziplevel: 6
  ngm:
    sensitivity: 0.5
  adapterremoval:
    __default__:  # global defaults. modify per qc_type
      adapter_file: "rawdata/adapters.txt"
      minqual: 20
      qualenc: 33
      maxqualval: 45
  kwip:
    kmer_size: 21
    sketch_size: 300000000
  mash:
    kmer_size: 21
    sketch_size: 100000
  varcall:
    # Per-aligner minimum MAPQ thresholds for using a read.
    minmapq:
      bwa: 30  # bwa scores approximately follow a PHRED scale (-10*log10(p))
      ngm: 10  # NGM scores are bonkers, and don't follow a particularly clear scale. In practice ~10 seems appropriate

    # Minimum base quality to count *base* in pileup
    minbq: 15 
    
    # Coverage per region across all samples. This is used to partition the
    # genome in a coverage-aware manner with goleft indexcov.
    # The values below are stupidly low for testing purposes, you should
    # increase the to at least the numbers in the comments.
    region_coverage_threshold:
      mpileup:    5  # 10000
      freebayes:  5  #  1000

    # Stop counting reads after this many. Per variant caller. It depends on the tool as to whether or not this is per-sample or per-file.
    max_depth:
      mpileup:   12000    # Per file
      freebayes: 12000    # Per file

    # Filters. These are series of command line arguments to pass to bcftools
    # view. These filters apply while multiallelic variants have been decomposed
    # into multiple overlapping variant calls. This allows e.g. allele frequency
    # filters to be performed on a per-allele basis.
    filters:
      default: >
        -i 'QUAL >= 10 &&
            INFO/DP >= 5 &&
            INFO/AN >= 3'


#######################################################################
#                           Resource Config                           #
#######################################################################
# TODO split this to a separate file, and make a snakemake rule to generate the defaults.
resources:
  # set to true to see all the resources for each rule
  __DEBUG__: false
  __default__:
    # A default set of resources. Jobs not configured individually will inherit these values.
    cores: 1
    disk_mb: 32000
    mem_mb: 1000
    time_min: 120
    localrule: 0  # this must be an integer, so 0 = false & 1 = true
  __max__:
    # A maximum of each resource permitted by your execution environment. Each job's
    # request will be capped at these values.
    # 
    # On cloud environments, use the maximum available resources of your machine type. On
    # HPC clusters, use the size of an individual node. On a local machine, use the size
    # of the machine you run snakemake on.
    cores: 32
    disk_mb: 300000
    mem_mb:  120000
    time_min: 2880
  # What follows overrides both the defaults above and any per-job specification in the 
  # snakemake rules files. The keys of this should be individual rule names as passed to
  # configure_resources() in the snakemake rules files. Run snakemake -np with the
  # __DEBUG__ variable above set to true to see a full list of the defaults.
  example_rule_name:
    cores: 8
    disk_mb: 300000
    mem_mb: 16000
    time_min: 120

# Unlike other code files in Acathophis, this file is placed in the public
# domain, so there are no restrictions on its modification. Specifically, these
# example files are licensed under the Creative Commons Zero licence, as that
# is a more portable concept of "public domain".
