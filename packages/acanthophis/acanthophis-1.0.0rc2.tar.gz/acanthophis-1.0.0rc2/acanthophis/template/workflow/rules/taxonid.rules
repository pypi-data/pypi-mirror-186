# These rules are part of Acanthophis. See https://github.com/kdm9/Acanthophis.
# This file *could* be modified, but then be careful when you update them. And
# please, if you find a bug, raise an issue on github so the fix gets shared
# with everyone.
#
# Copyright 2016-2022 Kevin Murray/Gekkonid Consulting
#
# This Source Code Form is subject to the terms of the Mozilla Public License,
# v. 2.0. If a copy of the MPL was not distributed with this file, You can
# obtain one at http://mozilla.org/MPL/2.0/.

#######################################################################
#                               KRAKEN                                #
#######################################################################

ruleorder: kraken_noreads > kraken_reads
rule kraken_noreads:
    input:
        reads=P("reads/samples/{sample}.fastq.gz"),
        hash=lambda wc: R(config["data_paths"]["kraken"][wc.db] + "/hash.k2d", keep_local=True),
        opts=lambda wc: R(config["data_paths"]["kraken"][wc.db] + "/opts.k2d", keep_local=True),
        taxo=lambda wc: R(config["data_paths"]["kraken"][wc.db] + "/taxo.k2d", keep_local=True),
    output:
        outtxt=P("taxonid/kraken/{db}~{sample}_output.txt.zst"),
        report=P("taxonid/kraken/{db}~{sample}_report.txt"),
    log: L("taxonid/kraken/{db}~{sample}.log"),
    resources: **rule_resources(config, "kraken_noreads", time_min=90, mem_gb=100, disk_gb=24, cores=4)
    conda: "envs/kraken.yml"
    params:
        ziplevel=int(config.get("tool_settings", {}).get('ziplevel', 6)) + 2,
    shell:
        "kraken2"
        "   --db $(dirname {input.hash})"
        "   --memory-mapping"
        "   --threads {threads}"
        "   --use-names"
        "   --report-minimizer-data"
        "   --report {output.report}"
        "   --output >(zstd -T{threads} -{params.ziplevel} >{output.outtxt})"
        "   {input.reads}"
        "   >{log} 2>&1"

rule kraken_reads:
    input:
        reads=P("reads/samples/{sample}.fastq.gz"),
        hash=lambda wc: R(config["data_paths"]["kraken"][wc.db] + "/hash.k2d", keep_local=True),
        opts=lambda wc: R(config["data_paths"]["kraken"][wc.db] + "/opts.k2d", keep_local=True),
        taxo=lambda wc: R(config["data_paths"]["kraken"][wc.db] + "/taxo.k2d", keep_local=True),
    output:
        fastq_un=P("taxonid/kraken/{db}~{sample}_unclassified.fastq.gz"),
        fastq_cl=P("taxonid/kraken/{db}~{sample}_classified.fastq.gz"),
        outtxt=P("taxonid/kraken/{db}/{sample}_output.txt.zst"),
        report=P("taxonid/kraken/{db}~{sample}_report.txt"),
    log: L("taxonid/kraken/{db}~{sample}.log"),
    resources: **rule_resources(config, "kraken_reads", time_min=90, mem_gb=100, disk_gb=24, cores=4)
    conda: "envs/kraken.yml"
    params:
        ziplevel=config.get("tool_settings", {}).get('ziplevel', 6),
    shell:
        "kraken2"
        "   --db $(dirname {input.hash})"
        "   --memory-mapping"
        "   --threads {threads}"
        "   --use-names"
        "   --report-minimizer-data"
        "   --report {output.report}"
        "   --classified-out >(gzip -{params.ziplevel} >{output.fastq_cl})"
        "   --unclassified-out >(gzip -{params.ziplevel} >{output.fastq_un})"
        "   --output >(zstd -T{threads} -{params.ziplevel} >{output.outtxt})"
        "   {input.reads}"
        "   >{log} 2>&1"


rule multiqc_kraken:
    input:
        lambda wc: P(expand("taxonid/kraken/{db}~{sample}_report.txt",
                            sample=config["SAMPLESETS"][wc.sampleset],
                            db=wc.db,
                            sampleset=wc.sampleset,
                   ))
    output:
        html=P("stats/multiqc/kraken_{db}~{sampleset}_multiqc.html"),
    log: L("stats/multiqc/kraken_{db}~{sampleset}_multiqc.log"),
    conda: "envs/qcstats.yml"
    resources: **rule_resources(config, "multiqc_kraken", time_min=30, mem_gb=2)
    shell:
        "multiqc"
        "   --no-megaqc-upload"
        "   --interactive"
        "   --no-data-dir"
        "   --comment 'Kraken report for sample set {wildcards.sampleset}'"
        "   --filename {output.html}"
        "   {input}"
        " >{log} 2>&1"


#######################################################################
#                                KAIJU                                #
#######################################################################

rule kaiju:
    input:
        reads=P("reads/samples/{sample}.fastq.gz"),
        taxon=lambda wc: R(config["data_paths"]["kaiju"][wc.db]["nodes"], keep_local=True),
        index=lambda wc: R(config["data_paths"]["kaiju"][wc.db]["fmi"], keep_local=True),
    output:
        P("taxonid/kaiju/{db}~{sample}.txt.zst"),
    log: L("taxonid/kaiju/{db}~{sample}.log"),
    resources: **rule_resources(config, "kaiju", time_min=90, mem_gb=32, disk_gb=24, cores=4)
    conda: "envs/kaiju.yml"
    params:
        ziplevel=int(config.get("tool_settings", {}).get('ziplevel', 6)) + 2,
    shell:
        "kaiju"
        "   -t {input.taxon}"
        "   -f {input.index}"
        "   -o >(zstd -T{threads} -{params.ziplevel} >{output})"
        "   -z {threads}"
        "   -i {input.reads}"
        "   >{log} 2>&1"


#######################################################################
#                               GraftM                                #
#######################################################################

rule graftm:
    input:
        reads=P("reads/samples/{sample}.fastq.gz"),
        package=lambda wc: R(config["data_paths"]["graftm"][wc.db], keep_local=True),
    output:
        directory(P("taxonid/graftm/{db}/{db}~{sample}/")),
    log: L("taxonid/graftm/{db}/{db}~{sample}.log"),
    resources: **rule_resources(config, "graftm", time_min=90, mem_gb=32, disk_gb=24, cores=4)
    conda: "envs/graftm.yml"
    shell:
        "graftM graft"
        "   --interleaved {input.reads}"
        "   --graftm_package {input.package}"
        "   --threads {threads}"
        "   --output_directory {output}"
        "   --force"
        "   --no_merge_reads"
        "   --log {log}"


#######################################################################
#                             CENTRIFUGE                              #
#######################################################################

rule centrifuge:
    input:
        db=lambda wc: R(config["data_paths"]["centrifuge"][wc.db], keep_local=True),
        r1=T("reads/samples/{sample}_R1.fastq.gz"),
        r2=T("reads/samples/{sample}_R2.fastq.gz"),
        se=T("reads/samples/{sample}_se.fastq.gz"),
    output:
        out=P("taxonid/centrifuge/{db}~{sample}.txt.zst"),
        report=P("taxonid/centrifuge/{db}~{sample}_report.txt"),
        metrics=P("taxonid/centrifuge/{db}~{sample}_metrics.txt"),
    log: L("taxonid/centrifuge/{db}~{sample}.log"),
    resources: **rule_resources(config, "centrifuge", time_min=90, mem_gb=16, disk_gb=8, cores=16)
    conda: "envs/centrifuge.yml"
    params:
        idx_prefix=lambda wc, input: stripext(input.db, [".1.cf", ".2.cf", ".3.cf", ".4.cf"]),
        ziplevel=int(config.get("tool_settings", {}).get('ziplevel', 6)) + 2,
    shell:
        "centrifuge"
        "   --shmem"
        "   --met-file {output.metrics}"
        "   --report-file {output.report}"
        "   -S >(zstd -T{threads} -{params.ziplevel} >{output.out})"
        "   --threads {threads}"
        "   -x {params.idx_prefix}"
        "   -1 {input.r1}"
        "   -2 {input.r2}"
        "   -U {input.se}"
        "   >{log} 2>&1"



#######################################################################
#                               Targets                               #
#######################################################################


rule all_graftm:
    input:
        [P(expand("taxonid/graftm/{db}/{db}~{sample}/",
                  sample=config["SAMPLESETS"][sampleset],
                  db=config["samplesets"][sampleset].get("graftm", {}).get("packages", []),
                  ))
         for sampleset in config["samplesets"]
         if "graftm" in config["samplesets"][sampleset]
        ],

rule all_kaiju:
    input:
        [P(expand("taxonid/kaiju/{db}~{sample}.txt.zst",
                  sample=config["SAMPLESETS"][sampleset],
                  db=config["samplesets"][sampleset].get("kaiju", {}).get("dbs", []),
                  ))
         for sampleset in config["samplesets"]
         if "kaiju" in config["samplesets"][sampleset]
        ],

rule all_kraken:
    input:
        P([f"stats/multiqc/kraken_{db}~{sampleset}_multiqc.html"
           for sampleset in config["samplesets"]
           for db in config["samplesets"][sampleset].get("kraken", {}).get("dbs", [])
           if "kraken" in config["samplesets"][sampleset]
        ]),
        [P(expand("taxonid/kraken/{db}~{sample}_report.txt",
                  sample=config["SAMPLESETS"][sampleset],
                  db=config["samplesets"][sampleset].get("kraken", {}).get("dbs", []),
                  ))
         for sampleset in config["samplesets"]
         if "kraken" in config["samplesets"][sampleset]
        ],
        [P(expand("taxonid/kraken/{db}~{sample}_unclassified.fastq.gz",
                  sample=config["SAMPLESETS"][sampleset],
                  db=config["samplesets"][sampleset].get("kraken", {}).get("dbs", []),
                  ))
         for sampleset in config["samplesets"]
         if config["samplesets"][sampleset].get("kraken", {}).get("reads", False)
        ]



rule all_centrifuge:
    input:
        [P(expand("taxonid/centrifuge/{db}~{sample}.txt.zst",
                  sample=config["SAMPLESETS"][sampleset],
                  db=config["samplesets"][sampleset].get("centrifuge", {}).get("dbs", []),
                  ))
         for sampleset in config["samplesets"]
         if "centrifuge" in config["samplesets"][sampleset]
        ],


rule all_taxonid:
    input:
        rules.all_kraken.input,
        rules.all_kaiju.input,
        rules.all_centrifuge.input,
