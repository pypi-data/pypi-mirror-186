import numpy as np
from scipy.linalg import qr

from cobyqa.utils import max_abs_arrays


def bound_constrained_tangential_step(grad, hess_prod, xl, xu, delta, debug, **kwargs):
    r"""
    Minimize approximately a quadratic function subject to bound constraints in
    a trust region.

    This function solves approximately

    .. math::

        \begin{aligned}
            \min_{d \in \R^n}   & \quad g^{\T}d + \frac{1}{2} d^{\T}Hd\\
            \text{s.t.}         & \quad l \le d \le u,\\
                                & \quad \norm{d} \le \Delta,
        \end{aligned}

    using an active-set variation of the truncated conjugate gradient method.

    Parameters
    ----------
    grad : numpy.ndarray, shape (n,)
        Gradient :math:`g` as shown above.
    hess_prod : callable
        Product of the Hessian matrix :math:`H` with any vector.

            ``hess_prod(d) -> numpy.ndarray, shape (n,)``

        returns the product :math:`Hd`.
    xl : numpy.ndarray, shape (n,)
        Lower bounds :math:`l` as shown above.
    xu : numpy.ndarray, shape (n,)
        Upper bounds :math:`u` as shown above.
    delta : float
        Trust-region radius :math:`\Delta` as shown above.
    debug : bool
        Whether to make debugging tests during the execution.

    Returns
    -------
    numpy.ndarray, shape (n,)
        Approximate solution :math:`d`.

    Other Parameters
    ----------------
    improve : bool, optional
        If ``True``, a solution generated by the truncated conjugate gradient
        method that is on the boundary of the trust region is improved by moving
        around the trust-region boundary on the two-dimensional space spanned by
        the solution and the gradient of the quadratic function at the solution
        (default is True).

    Notes
    -----
    This function implements Algorithm 6.2 of [1]_. It is assumed that the
    origin is feasible with respect to the bound constraints `xl` and `xu`, and
    that `delta` is finite and positive.

    References
    ----------
    .. [1] T. M. Ragonneau. "Model-Based Derivative-Free Optimization Methods
       and Software." Ph.D. thesis. Hong Kong: Department of Applied
       Mathematics, The Hong Kong Polytechnic University, 2022.
    """
    # Copy the arrays that may be modified by the code below.
    n = grad.size
    grad = np.copy(grad)

    # Check the feasibility of the subproblem.
    if debug:
        tol = 10.0 * np.finfo(float).eps * n * max_abs_arrays(xl, xu)
        assert np.all(xl <= tol)
        assert np.all(xu >= -tol)
        assert np.isfinite(delta) and delta > 0.0
    xl = np.minimum(xl, 0.0)
    xu = np.maximum(xu, 0.0)

    # Calculate the initial active set.
    free_bd = ((xl < 0.0) | (grad < 0.0)) & ((xu > 0.0) | (grad > 0.0))

    # Set the initial iterate and the initial search direction.
    step = np.zeros_like(grad)
    sd = np.zeros_like(step)
    sd[free_bd] = -grad[free_bd]

    k = 0
    reduct = 0.0
    boundary_reached = False
    while k < np.count_nonzero(free_bd):
        # Stop the computations if sd is not a descent direction.
        grad_sd = np.inner(grad, sd)
        if grad_sd >= 0.0:
            break

        # Set alpha_tr to the step size for the trust-region constraint.
        try:
            alpha_tr = _get_alpha_tr(step, sd, delta)
        except ZeroDivisionError:
            break

        # Stop the computations if a step along sd is expected to give a
        # relatively small reduction in the objective function.
        if -alpha_tr * grad_sd <= 1e-2 * reduct:
            break

        # Set alpha_quad to the step size for the minimization problem.
        hess_sd = hess_prod(sd)
        curv_sd = np.inner(sd, hess_sd)
        if curv_sd > np.finfo(float).tiny * abs(grad_sd):
            alpha_quad = max(-grad_sd / curv_sd, 0.0)
        else:
            alpha_quad = np.inf

        # Stop the computations if the reduction in the objective function
        # provided by an unconstrained step is small.
        alpha = min(alpha_tr, alpha_quad)
        if -alpha * (grad_sd + 0.5 * alpha * curv_sd) <= 1e-2 * reduct:
            break

        # Set alpha_bd to the step size for the bound constraints.
        i_xl = (xl > -np.inf) & (sd < -np.finfo(float).tiny * np.abs(xl - step))
        i_xu = (xu < np.inf) & (sd > np.finfo(float).tiny * np.abs(xu - step))
        all_alpha_xl = np.full_like(step, np.inf)
        all_alpha_xu = np.full_like(step, np.inf)
        all_alpha_xl[i_xl] = np.maximum((xl[i_xl] - step[i_xl]) / sd[i_xl], 0.0)
        all_alpha_xu[i_xu] = np.maximum((xu[i_xu] - step[i_xu]) / sd[i_xu], 0.0)
        alpha_xl = np.min(all_alpha_xl)
        alpha_xu = np.min(all_alpha_xu)
        alpha_bd = min(alpha_xl, alpha_xu)

        # Update the iterate.
        alpha = min(alpha, alpha_bd)
        if alpha > 0.0:
            step[free_bd] = np.maximum(xl[free_bd], np.minimum(step[free_bd] + alpha * sd[free_bd], xu[free_bd]))
            grad += alpha * hess_sd
            reduct -= alpha * (grad_sd + 0.5 * alpha * curv_sd)

        if alpha < min(alpha_tr, alpha_bd):
            # The current iteration is a conjugate gradient iteration. Update
            # the search direction so that it is conjugate (with respect to H)
            # to all the previous search directions.
            beta = np.inner(grad[free_bd], hess_sd[free_bd]) / curv_sd
            sd[free_bd] = beta * sd[free_bd] - grad[free_bd]
            sd[~free_bd] = 0.0
            k += 1
        elif alpha < alpha_tr:
            # The iterate is restricted by a bound constraint. Add this bound
            # constraint to the active set, and restart the calculations.
            if alpha_xl <= alpha:
                i_new = np.argmin(all_alpha_xl)
                step[i_new] = xl[i_new]
            else:
                i_new = np.argmin(all_alpha_xu)
                step[i_new] = xu[i_new]
            free_bd[i_new] = False
            sd[free_bd] = -grad[free_bd]
            sd[~free_bd] = 0.0
            k = 0
        else:
            # The current iterate is on the trust-region boundary. Add all the
            # active bounds to the working set to prepare for the improvement of
            # the solution, and stop the iterations.
            if alpha_xl <= alpha:
                i_new = _argmin(all_alpha_xl)
                step[i_new] = xl[i_new]
                free_bd[i_new] = False
            if alpha_xu <= alpha:
                i_new = _argmin(all_alpha_xu)
                step[i_new] = xu[i_new]
                free_bd[i_new] = False
            boundary_reached = True
            break

    # Attempt to improve the solution on the trust-region boundary.
    if kwargs.get("improve", True) and boundary_reached:
        while np.count_nonzero(free_bd) > 0:
            # Check whether a substantial reduction in the objective function is
            # possible, and set the search direction.
            step_sq = np.inner(step[free_bd], step[free_bd])
            grad_sq = np.inner(grad[free_bd], grad[free_bd])
            grad_step = np.inner(grad[free_bd], step[free_bd])
            grad_sd = -np.sqrt(max(step_sq * grad_sq - grad_step ** 2.0, 0.0))
            sd[free_bd] = grad_step * step[free_bd] - step_sq * grad[free_bd]
            sd[~free_bd] = 0.0
            if grad_sd >= -1e-2 * reduct or np.any(grad_sd >= -np.finfo(float).tiny * np.abs(sd[free_bd])):
                break
            sd[free_bd] /= -grad_sd

            # Calculate an upper bound for the tangent of half the angle theta
            # of this alternative iteration. The step will be updated as:
            # step = cos(theta) * step + sin(theta) * sd.
            temp_xl = np.zeros(n)
            temp_xu = np.zeros(n)
            temp_xl[free_bd] = np.square(step[free_bd]) + np.square(sd[free_bd]) - np.square(xl[free_bd])
            temp_xu[free_bd] = np.square(step[free_bd]) + np.square(sd[free_bd]) - np.square(xu[free_bd])
            temp_xl[temp_xl > 0.0] = np.sqrt(temp_xl[temp_xl > 0.0]) - sd[temp_xl > 0.0]
            temp_xu[temp_xu > 0.0] = np.sqrt(temp_xu[temp_xu > 0.0]) + sd[temp_xu > 0.0]
            dist_xl = np.maximum(step - xl, 0.0)
            dist_xu = np.maximum(xu - step, 0.0)
            i_xl = temp_xl > np.finfo(float).tiny * dist_xl
            i_xu = temp_xu > np.finfo(float).tiny * dist_xu
            all_t_xl = np.ones(n)
            all_t_xu = np.ones(n)
            all_t_xl[i_xl] = np.minimum(all_t_xl[i_xl], dist_xl[i_xl] / temp_xl[i_xl])
            all_t_xu[i_xu] = np.minimum(all_t_xu[i_xu], dist_xu[i_xu] / temp_xu[i_xu])
            t_xl = np.min(all_t_xl)
            t_xu = np.min(all_t_xu)
            t_bd = min(t_xl, t_xu)

            # Calculate some curvature information.
            hess_step = hess_prod(step)
            hess_sd = hess_prod(sd)
            curv_step = np.inner(step, hess_step)
            curv_sd = np.inner(sd, hess_sd)
            curv_step_sd = np.inner(step, hess_sd)

            # For a range of equally spaced values of tan(0.5 * theta),
            # calculate the reduction in the objective function that would be
            # obtained by accepting the corresponding angle.
            n_samples = 20
            n_samples = int((n_samples - 3) * t_bd + 3)
            t_samples = np.linspace(t_bd / n_samples, t_bd, n_samples)
            sin_values = 2.0 * t_samples / (1.0 + np.square(t_samples))
            all_reduct = sin_values * (grad_step * t_samples - grad_sd - t_samples * curv_step + sin_values * (t_samples * curv_step_sd - 0.5 * (curv_sd - curv_step)))
            if np.all(all_reduct <= 0.0):
                # No reduction in the objective function is obtained.
                break

            # Accept the angle that provides the largest reduction in the
            # objective function, and update the iterate.
            i_max = np.argmax(all_reduct)
            cos_value = (1.0 - t_samples[i_max] ** 2.0) / (1.0 + t_samples[i_max] ** 2.0)
            step[free_bd] = cos_value * step[free_bd] + sin_values[i_max] * sd[free_bd]
            # assert np.all(xl - 1e-15 <= step)
            # assert np.all(step <= xu + 1e-15)
            grad += (cos_value - 1.0) * hess_step + sin_values[i_max] * hess_sd
            reduct += all_reduct[i_max]

            # If the above angle is restricted by bound constraints, add them to
            # the working set, and restart the alternative iteration. Otherwise,
            # the calculations are terminated.
            if t_bd < 1.0 and i_max == n_samples - 1:
                if t_xl <= t_bd:
                    i_new = _argmin(all_t_xl)
                    step[i_new] = xl[i_new]
                    free_bd[i_new] = False
                if t_xu <= t_bd:
                    i_new = _argmin(all_t_xu)
                    step[i_new] = xu[i_new]
                    free_bd[i_new] = False
            else:
                break

    if debug:
        assert np.all(xl <= step)
        assert np.all(step <= xu)
        assert np.linalg.norm(step) < 1.1 * delta
    return step


def linearly_constrained_tangential_step(grad, hess_prod, xl, xu, aub, bub, aeq, delta, debug, **kwargs):
    r"""
    Minimize approximately a quadratic function subject to bound and linear
    constraints in a trust region.

    This function solves approximately

    .. math::

        \begin{aligned}
            \min_{d \in \R^n}   & \quad g^{\T}d + \frac{1}{2} d^{\T}Hd\\
            \text{s.t.}         & \quad l \le d \le u,\\
                                & \quad A_{\text{ub}}d \le b_{\text{ub}},\\
                                & \quad A_{\text{eq}}d = 0,\\
                                & \quad \norm{d} \le \Delta,
        \end{aligned}

    using an active-set variation of the truncated conjugate gradient method.

    Parameters
    ----------
    grad : numpy.ndarray, shape (n,)
        Gradient :math:`g` as shown above.
    hess_prod : callable
        Product of the Hessian matrix :math:`H` with any vector.

            ``hess_prod(d) -> numpy.ndarray, shape (n,)``

        returns the product :math:`Hd`.
    xl : numpy.ndarray, shape (n,)
        Lower bounds :math:`l` as shown above.
    xu : numpy.ndarray, shape (n,)
        Upper bounds :math:`u` as shown above.
    aub : numpy.ndarray, shape (m_linear_ub, n)
        Coefficient matrix :math:`A_{\text{ub}}` as shown above.
    bub : numpy.ndarray, shape (m_linear_ub,)
        Right-hand side :math:`b_{\text{ub}}` as shown above.
    aeq : numpy.ndarray, shape (m_linear_eq, n)
        Coefficient matrix :math:`A_{\text{eq}}` as shown above.
    delta : float
        Trust-region radius :math:`\Delta` as shown above.
    debug : bool
        Whether to make debugging tests during the execution.

    Returns
    -------
    numpy.ndarray, shape (n,)
        Approximate solution :math:`d`.

    Other Parameters
    ----------------
    improve : bool, optional
        If ``True``, a solution generated by the truncated conjugate gradient
        method that is on the boundary of the trust region is improved by moving
        around the trust-region boundary on the two-dimensional space spanned by
        the solution and the gradient of the quadratic function at the solution
        (default is True).

    Notes
    -----
    It is assumed that the origin is feasible with respect to the bound and the
    linear constraints `xl` and `xu`, and that `delta` is finite and positive.
    """
    # Copy the arrays that may be modified by the code below.
    n = grad.size
    grad = np.copy(grad)

    # Check the feasibility of the subproblem.
    tol = 10.0 * np.finfo(float).eps * n * max_abs_arrays(xl, xu)
    if debug:
        assert np.all(xl <= tol)
        assert np.all(xu >= -tol)
        assert np.all(bub >= -tol)
        assert np.isfinite(delta) and delta > 0.0
    xl = np.minimum(xl, 0.0)
    xu = np.maximum(xu, 0.0)
    bub = np.maximum(bub, 0.0)

    # Calculate the initial active set.
    free_xl = (xl < 0.0) | (grad < 0.0)
    free_xu = (xu > 0.0) | (grad > 0.0)
    free_ub = (bub > 0.0) | (np.dot(aub, grad) > 0.0)
    n_act, q = get_qr_tangential(aub, aeq, free_xl, free_xu, free_ub)

    # Set the initial iterate and the initial search direction.
    step = np.zeros_like(grad)
    sd = np.dot(q[:, n_act:], np.dot(q[:, n_act:].T, -grad))
    resid = np.copy(bub)

    k = 0
    reduct = 0.0
    boundary_reached = False
    while k < n - n_act:
        # Stop the computations if sd is not a descent direction.
        grad_sd = np.inner(grad, sd)
        if grad_sd >= 0.0:
            break

        # Set alpha_tr to the step size for the trust-region constraint.
        try:
            alpha_tr = _get_alpha_tr(step, sd, delta)
        except ZeroDivisionError:
            break

        # Stop the computations if a step along sd is expected to give a
        # relatively small reduction in the objective function.
        if -alpha_tr * grad_sd <= 1e-2 * reduct:
            break

        # Set alpha_quad to the step size for the minimization problem.
        hess_sd = hess_prod(sd)
        curv_sd = np.inner(sd, hess_sd)
        if curv_sd > np.finfo(float).tiny * abs(grad_sd):
            alpha_quad = max(-grad_sd / curv_sd, 0.0)
        else:
            alpha_quad = np.inf

        # Stop the computations if the reduction in the objective function
        # provided by an unconstrained step is small.
        alpha = min(alpha_tr, alpha_quad)
        if -alpha * (grad_sd + 0.5 * alpha * curv_sd) <= 1e-2 * reduct:
            break

        # Set alpha_bd to the step size for the bound constraints.
        i_xl = free_xl & (xl > -np.inf) & (sd < -np.finfo(float).tiny * np.abs(xl - step))
        i_xu = free_xu & (xu < np.inf) & (sd > np.finfo(float).tiny * np.abs(xu - step))
        all_alpha_xl = np.full_like(step, np.inf)
        all_alpha_xu = np.full_like(step, np.inf)
        all_alpha_xl[i_xl] = np.maximum((xl[i_xl] - step[i_xl]) / sd[i_xl], 0.0)
        all_alpha_xu[i_xu] = np.maximum((xu[i_xu] - step[i_xu]) / sd[i_xu], 0.0)
        alpha_xl = np.min(all_alpha_xl)
        alpha_xu = np.min(all_alpha_xu)
        alpha_bd = min(alpha_xl, alpha_xu)

        # Set alpha_ub to the step size for the linear constraints.
        aub_sd = np.dot(aub, sd)
        i_ub = free_ub & (aub_sd > np.finfo(float).tiny * np.abs(resid))
        all_alpha_ub = np.full_like(bub, np.inf)
        all_alpha_ub[i_ub] = resid[i_ub] / aub_sd[i_ub]
        alpha_ub = np.min(all_alpha_ub, initial=np.inf)

        # Update the iterate.
        alpha = min(alpha, alpha_bd, alpha_ub)
        if alpha > 0.0:
            step = np.maximum(xl, np.minimum(step + alpha * sd, xu))
            grad += alpha * hess_sd
            resid = np.maximum(0.0, resid - alpha * aub_sd)
            reduct -= alpha * (grad_sd + 0.5 * alpha * curv_sd)

        if alpha < min(alpha_tr, alpha_bd, alpha_ub):
            # The current iteration is a conjugate gradient iteration. Update
            # the search direction so that it is conjugate (with respect to H)
            # to all the previous search directions.
            grad_proj = np.dot(q[:, n_act:], np.dot(q[:, n_act:].T, grad))
            beta = np.inner(grad_proj, hess_sd) / curv_sd
            sd = beta * sd - grad_proj
            k += 1
        elif alpha < alpha_tr:
            # The iterate is restricted by a bound/linear constraint. Add this
            # constraint to the active set, and restart the calculations.
            if alpha_xl <= alpha:
                i_new = np.argmin(all_alpha_xl)
                step[i_new] = xl[i_new]
                free_xl[i_new] = False
            elif alpha_xu <= alpha:
                i_new = np.argmin(all_alpha_xu)
                step[i_new] = xu[i_new]
                free_xu[i_new] = False
            else:
                i_new = np.argmin(all_alpha_ub)
                free_ub[i_new] = False
            n_act, q = get_qr_tangential(aub, aeq, free_xl, free_xu, free_ub)
            sd = np.dot(q[:, n_act:], np.dot(q[:, n_act:].T, -grad))
            k = 0
        else:
            # The current iterate is on the trust-region boundary. Add all the
            # active bound/linear constraints to the working set to prepare for
            # the improvement of the solution, and stop the iterations.
            if alpha_xl <= alpha:
                i_new = _argmin(all_alpha_xl)
                step[i_new] = xl[i_new]
                free_xl[i_new] = False
            if alpha_xu <= alpha:
                i_new = _argmin(all_alpha_xu)
                step[i_new] = xu[i_new]
                free_xu[i_new] = False
            if alpha_ub <= alpha:
                i_new = _argmin(all_alpha_ub)
                free_ub[i_new] = False
            n_act, q = get_qr_tangential(aub, aeq, free_xl, free_xu, free_ub)
            boundary_reached = True
            break

    # Attempt to improve the solution on the trust-region boundary.
    if kwargs.get("improve", True) and boundary_reached:
        while n_act < n:
            # Check whether a substantial reduction in the objective function is
            # possible, and set the search direction.
            step_proj = np.dot(q[:, n_act:], np.dot(q[:, n_act:].T, step))
            grad_proj = np.dot(q[:, n_act:], np.dot(q[:, n_act:].T, grad))
            step_sq = np.inner(step_proj, step_proj)
            grad_sq = np.inner(grad_proj, grad_proj)
            grad_step = np.inner(grad_proj, step_proj)
            grad_sd = -np.sqrt(max(step_sq * grad_sq - grad_step ** 2.0, 0.0))
            sd = np.dot(q[:, n_act:], np.dot(q[:, n_act:].T, grad_step * step - step_sq * grad))
            if grad_sd >= -1e-2 * reduct or np.any(grad_sd >= -np.finfo(float).tiny * np.abs(sd)):
                break
            sd /= -grad_sd

            # Calculate an upper bound for the tangent of half the angle theta
            # of this alternative iteration for the bound constraints. The step
            # will be updated as:
            # step += (cos(theta) - 1) * step_proj + sin(theta) * sd.
            temp_xl = np.zeros(n)
            temp_xu = np.zeros(n)
            dist_xl = np.maximum(step - xl, 0.0)
            dist_xu = np.maximum(xu - step, 0.0)
            temp_xl[free_xl] = np.square(sd[free_xl]) - dist_xl[free_xl] * (dist_xl[free_xl] - 2.0 * step_proj[free_xl])
            temp_xu[free_xu] = np.square(sd[free_xu]) - dist_xu[free_xu] * (dist_xu[free_xu] + 2.0 * step_proj[free_xu])
            temp_xl[temp_xl > 0.0] = np.sqrt(temp_xl[temp_xl > 0.0]) - sd[temp_xl > 0.0]
            temp_xu[temp_xu > 0.0] = np.sqrt(temp_xu[temp_xu > 0.0]) + sd[temp_xu > 0.0]
            i_xl = temp_xl > np.finfo(float).tiny * dist_xl
            i_xu = temp_xu > np.finfo(float).tiny * dist_xu
            all_t_xl = np.ones(n)
            all_t_xu = np.ones(n)
            all_t_xl[i_xl] = np.minimum(all_t_xl[i_xl], dist_xl[i_xl] / temp_xl[i_xl])
            all_t_xu[i_xu] = np.minimum(all_t_xu[i_xu], dist_xu[i_xu] / temp_xu[i_xu])
            t_xl = np.min(all_t_xl)
            t_xu = np.min(all_t_xu)
            t_bd = min(t_xl, t_xu)

            # Calculate an upper bound for the tangent of half the angle theta
            # of this alternative iteration for the linear constraints.
            temp_ub = np.zeros_like(resid)
            aub_step = np.dot(aub, step_proj)
            aub_sd = np.dot(aub, sd)
            temp_ub[free_ub] = np.square(aub_sd[free_ub]) - resid[free_ub] * (resid[free_ub] + 2.0 * aub_step[free_ub])
            temp_ub[temp_ub > 0.0] = np.sqrt(temp_ub[temp_ub > 0.0]) + aub_sd[temp_ub > 0.0]
            i_ub = temp_ub > np.finfo(float).tiny * resid
            all_t_ub = np.ones_like(resid)
            all_t_ub[i_ub] = np.minimum(all_t_ub[i_ub], resid[i_ub] / temp_ub[i_ub])
            t_ub = np.min(all_t_ub, initial=1.0)
            t_min = min(t_bd, t_ub)

            # Calculate some curvature information.
            hess_step = hess_prod(step_proj)
            hess_sd = hess_prod(sd)
            curv_step = np.inner(step_proj, hess_step)
            curv_sd = np.inner(sd, hess_sd)
            curv_step_sd = np.inner(step_proj, hess_sd)

            # For a range of equally spaced values of tan(0.5 * theta),
            # calculate the reduction in the objective function that would be
            # obtained by accepting the corresponding angle.
            n_samples = 20
            n_samples = int((n_samples - 3) * t_min + 3)
            t_samples = np.linspace(t_min / n_samples, t_min, n_samples)
            sin_values = 2.0 * t_samples / (1.0 + np.square(t_samples))
            all_reduct = sin_values * (grad_step * t_samples - grad_sd - sin_values * (0.5 * np.square(t_samples) * curv_step - 2.0 * t_samples * curv_step_sd + 0.5 * curv_sd))
            if np.all(all_reduct <= 0.0):
                # No reduction in the objective function is obtained.
                break

            # Accept the angle that provides the largest reduction in the
            # objective function, and update the iterate.
            i_max = np.argmax(all_reduct)
            cos_value = (1.0 - t_samples[i_max] ** 2.0) / (1.0 + t_samples[i_max] ** 2.0)
            step = np.maximum(xl, np.minimum(step + (cos_value - 1.0) * step_proj + sin_values[i_max] * sd, xu))
            grad += (cos_value - 1.0) * hess_step + sin_values[i_max] * hess_sd
            resid = np.maximum(0.0, resid - (cos_value - 1.0) * aub_step - sin_values[i_max] * aub_sd)
            reduct += all_reduct[i_max]

            # If the above angle is restricted by bound constraints, add them to
            # the working set, and restart the alternative iteration. Otherwise,
            # the calculations are terminated.
            if t_min < 1.0 and i_max == n_samples - 1:
                if t_xl <= t_min:
                    i_new = _argmin(all_t_xl)
                    step[i_new] = xl[i_new]
                    free_xl[i_new] = False
                if t_xu <= t_min:
                    i_new = _argmin(all_t_xu)
                    step[i_new] = xu[i_new]
                    free_xl[i_new] = False
                if t_ub <= t_min:
                    i_new = _argmin(all_t_ub)
                    free_ub[i_new] = False
                n_act, q = get_qr_tangential(aub, aeq, free_xl, free_xu, free_ub)
            else:
                break

    if debug:
        assert np.all(xl <= step)
        assert np.all(step <= xu)
        assert np.all(np.dot(aub, step) <= bub + tol)
        assert np.all(np.abs(np.dot(aeq, step)) <= tol)
        assert np.linalg.norm(step) < 1.1 * delta
    return step


def bound_constrained_normal_step(aub, bub, aeq, beq, xl, xu, delta, debug, **kwargs):
    r"""
    Minimize approximately a linear constraint violation subject to bound
    constraints in a trust region.

    This function solves approximately

    .. math::

        \begin{aligned}
            \min_{d \in \R^n}   & \quad \frac{1}{2} \big(\norm{[A_{\text{ub}}d - b_{\text{ub}}]_+}^2 + \norm{A_{\text{eq}}d - b_{\text{eq}}}^2\big)\\
            \text{s.t.}         & \quad l \le d \le u,\\
                                & \quad \norm{d} \le \Delta,
        \end{aligned}

    using a variation of the truncated conjugate gradient method.

    Parameters
    ----------
    aub : numpy.ndarray, shape (m_linear_ub, n)
        Matrix :math:`A_{\text{ub}}` as shown above.
    bub : numpy.ndarray, shape (m_linear_ub,)
        Vector :math:`b_{\text{ub}}` as shown above.
    aeq : numpy.ndarray, shape (m_linear_eq, n)
        Matrix :math:`A_{\text{eq}}` as shown above.
    beq : numpy.ndarray, shape (m_linear_eq,)
        Vector :math:`b_{\text{eq}}` as shown above.
    xl : numpy.ndarray, shape (n,)
        Lower bounds :math:`l` as shown above.
    xu : numpy.ndarray, shape (n,)
        Upper bounds :math:`u` as shown above.
    delta : float
        Trust-region radius :math:`\Delta` as shown above.
    debug : bool
        Whether to make debugging tests during the execution.

    Returns
    -------
    numpy.ndarray, shape (n,)
        Approximate solution :math:`d`.

    Other Parameters
    ----------------
    improve : bool, optional
        If ``True``, a solution generated by the truncated conjugate gradient
        method that is on the boundary of the trust region is improved by moving
        around the trust-region boundary on the two-dimensional space spanned by
        the solution and the gradient of the quadratic function at the solution
        (default is True).

    Notes
    -----
    It is assumed that the origin is feasible with respect to the bound
    constraints `xl` and `xu`, and that `delta` is finite and positive.
    """
    # Check the feasibility of the subproblem.
    m_linear_ub, n = aub.shape
    if debug:
        tol = 10.0 * np.finfo(float).eps * n * max_abs_arrays(xl, xu)
        assert np.all(xl <= tol)
        assert np.all(xu >= -tol)
        assert np.isfinite(delta) and delta > 0.0
    xl = np.minimum(xl, 0.0)
    xu = np.maximum(xu, 0.0)

    # Calculate the initial active set.
    grad = np.r_[np.dot(aeq.T, -beq), np.maximum(0.0, -bub)]
    free_xl = (xl < 0.0) | (grad[:n] < 0.0)
    free_xu = (xu > 0.0) | (grad[:n] > 0.0)
    free_slack = bub < 0.0
    free_ub = (bub > 0.0) | (np.dot(aub, grad[:n]) - grad[n:] > 0.0)
    n_act, q = get_qr_normal(aub, free_xl, free_xu, free_slack, free_ub)

    # Calculate an upper bound on the norm of the slack variables. It is not
    # used in the original algorithm, but it may prevent undesired behaviors
    # engendered by computer rounding errors.
    delta_slack = np.sqrt(np.inner(beq, beq) + np.inner(grad[n:], grad[n:]))

    # Set the initial iterate and the initial search direction.
    step = np.zeros(n)
    sd = np.dot(q[:, n_act:], np.dot(q[:, n_act:].T, -grad))
    resid = bub + grad[n:]

    k = 0
    reduct = 0.0
    boundary_reached = False
    while k < n + m_linear_ub - n_act:
        # Stop the computations if sd is not a descent direction.
        grad_sd = np.inner(grad, sd)
        if grad_sd >= 0.0:
            break

        # Set alpha_tr to the step size for the trust-region constraint.
        try:
            alpha_tr = _get_alpha_tr(step, sd[:n], delta)
        except ZeroDivisionError:
            alpha_tr = np.inf

        # Prevent undesired behaviors engendered by computer rounding errors by
        # considering the trust-region constraint on the slack variables.
        try:
            alpha_tr = min(alpha_tr, _get_alpha_tr(grad[n:], sd[n:], delta_slack))
        except ZeroDivisionError:
            pass

        # Stop the computations if a step along sd is expected to give a
        # relatively small reduction in the objective function.
        if -alpha_tr * grad_sd <= 1e-2 * reduct:
            break

        # Set alpha_quad to the step size for the minimization problem.
        hess_sd = np.r_[np.dot(aeq.T, np.dot(aeq, sd[:n])), sd[n:]]
        curv_sd = np.inner(sd, hess_sd)
        if curv_sd > np.finfo(float).tiny * abs(grad_sd):
            alpha_quad = max(-grad_sd / curv_sd, 0.0)
        else:
            alpha_quad = np.inf

        # Stop the computations if the reduction in the objective function
        # provided by an unconstrained step is small.
        alpha = min(alpha_tr, alpha_quad)
        if -alpha * (grad_sd + 0.5 * alpha * curv_sd) <= 1e-2 * reduct:
            break

        # Set alpha_bd to the step size for the bound constraints.
        i_xl = free_xl & (xl > -np.inf) & (sd[:n] < -np.finfo(float).tiny * np.abs(xl - step))
        i_xu = free_xu & (xu < np.inf) & (sd[:n] > np.finfo(float).tiny * np.abs(xu - step))
        i_slack = free_slack & (sd[n:] < -np.finfo(float).tiny * np.abs(grad[n:]))
        all_alpha_xl = np.full_like(step, np.inf)
        all_alpha_xu = np.full_like(step, np.inf)
        all_alpha_slack = np.full_like(bub, np.inf)
        all_alpha_xl[i_xl] = np.maximum((xl[i_xl] - step[i_xl]) / sd[:n][i_xl], 0.0)
        all_alpha_xu[i_xu] = np.maximum((xu[i_xu] - step[i_xu]) / sd[:n][i_xu], 0.0)
        all_alpha_slack[i_slack] = np.maximum(-grad[n:][i_slack] / sd[n:][i_slack], 0.0)
        alpha_xl = np.min(all_alpha_xl)
        alpha_xu = np.min(all_alpha_xu)
        alpha_slack = np.min(all_alpha_slack, initial=np.inf)
        alpha_bd = min(alpha_xl, alpha_xu, alpha_slack)

        # Set alpha_ub to the step size for the linear constraints.
        aub_sd = np.dot(aub, sd[:n]) - sd[n:]
        i_ub = free_ub & (aub_sd > np.finfo(float).tiny * np.abs(resid))
        all_alpha_ub = np.full_like(bub, np.inf)
        all_alpha_ub[i_ub] = resid[i_ub] / aub_sd[i_ub]
        alpha_ub = np.min(all_alpha_ub, initial=np.inf)

        # Update the iterate.
        alpha = min(alpha, alpha_bd, alpha_ub)
        if alpha > 0.0:
            step = np.maximum(xl, np.minimum(step + alpha * sd[:n], xu))
            grad += alpha * hess_sd
            resid = np.maximum(0.0, resid - alpha * aub_sd)
            reduct -= alpha * (grad_sd + 0.5 * alpha * curv_sd)

        if alpha < min(alpha_tr, alpha_bd, alpha_ub):
            # The current iteration is a conjugate gradient iteration. Update
            # the search direction so that it is conjugate (with respect to H)
            # to all the previous search directions.
            grad_proj = np.dot(q[:, n_act:], np.dot(q[:, n_act:].T, grad))
            beta = np.inner(grad_proj, hess_sd) / curv_sd
            sd = beta * sd - grad_proj
            k += 1
        elif alpha < alpha_tr:
            # The iterate is restricted by a bound/linear constraint. Add this
            # constraint to the active set, and restart the calculations.
            if alpha_xl <= alpha:
                i_new = np.argmin(all_alpha_xl)
                step[i_new] = xl[i_new]
                free_xl[i_new] = False
            elif alpha_xu <= alpha:
                i_new = np.argmin(all_alpha_xu)
                step[i_new] = xu[i_new]
                free_xu[i_new] = False
            elif alpha_slack <= alpha:
                i_new = np.argmin(all_alpha_slack)
                free_slack[i_new] = False
            else:
                i_new = np.argmin(all_alpha_ub)
                free_ub[i_new] = False
            n_act, q = get_qr_normal(aub, free_xl, free_xu, free_slack, free_ub)
            sd = np.dot(q[:, n_act:], np.dot(q[:, n_act:].T, -grad))
            k = 0
        else:
            # The current iterate is on the trust-region boundary. Add all the
            # active bound constraints to the working set to prepare for the
            # improvement of the solution, and stop the iterations.
            if alpha_xl <= alpha:
                i_new = _argmin(all_alpha_xl)
                step[i_new] = xl[i_new]
                free_xl[i_new] = False
            if alpha_xu <= alpha:
                i_new = _argmin(all_alpha_xu)
                step[i_new] = xu[i_new]
                free_xu[i_new] = False
            boundary_reached = True
            break

    # Attempt to improve the solution on the trust-region boundary.
    if kwargs.get("improve", True) and boundary_reached:
        free_bd = free_xl & free_xu
        grad = np.dot(aub.T, np.maximum(np.dot(aub, step) - bub, 0.0)) + np.dot(aeq.T, (np.dot(aeq, step) - beq))
        sd = np.zeros(n)
        while np.count_nonzero(free_bd) > 0:
            # Check whether a substantial reduction in the objective function is
            # possible, and set the search direction.
            step_sq = np.inner(step[free_bd], step[free_bd])
            grad_sq = np.inner(grad[free_bd], grad[free_bd])
            grad_step = np.inner(grad[free_bd], step[free_bd])
            grad_sd = -np.sqrt(max(step_sq * grad_sq - grad_step ** 2.0, 0.0))
            sd[free_bd] = grad_step * step[free_bd] - step_sq * grad[free_bd]
            sd[~free_bd] = 0.0
            if grad_sd >= -1e-2 * reduct or np.any(grad_sd >= -np.finfo(float).tiny * np.abs(sd[free_bd])):
                break
            sd[free_bd] /= -grad_sd

            # Calculate an upper bound for the tangent of half the angle theta
            # of this alternative iteration. The step will be updated as:
            # step = cos(theta) * step + sin(theta) * sd.
            temp_xl = np.zeros(n)
            temp_xu = np.zeros(n)
            temp_xl[free_bd] = np.square(step[free_bd]) + np.square(sd[free_bd]) - np.square(xl[free_bd])
            temp_xu[free_bd] = np.square(step[free_bd]) + np.square(sd[free_bd]) - np.square(xu[free_bd])
            temp_xl[temp_xl > 0.0] = np.sqrt(temp_xl[temp_xl > 0.0]) - sd[temp_xl > 0.0]
            temp_xu[temp_xu > 0.0] = np.sqrt(temp_xu[temp_xu > 0.0]) + sd[temp_xu > 0.0]
            dist_xl = np.maximum(step - xl, 0.0)
            dist_xu = np.maximum(xu - step, 0.0)
            i_xl = temp_xl > np.finfo(float).tiny * dist_xl
            i_xu = temp_xu > np.finfo(float).tiny * dist_xu
            all_t_xl = np.ones(n)
            all_t_xu = np.ones(n)
            all_t_xl[i_xl] = np.minimum(all_t_xl[i_xl], dist_xl[i_xl] / temp_xl[i_xl])
            all_t_xu[i_xu] = np.minimum(all_t_xu[i_xu], dist_xu[i_xu] / temp_xu[i_xu])
            t_xl = np.min(all_t_xl)
            t_xu = np.min(all_t_xu)
            t_bd = min(t_xl, t_xu)

            # For a range of equally spaced values of tan(0.5 * theta),
            # calculate the reduction in the objective function that would be
            # obtained by accepting the corresponding angle.
            # TODO: Improve the computations below (possible?).
            n_samples = 20
            n_samples = int((n_samples - 3) * t_bd + 3)
            t_samples = np.linspace(t_bd / n_samples, t_bd, n_samples)
            resid_ub = np.maximum(np.dot(aub, step) - bub, 0.0)
            resid_eq = np.dot(aeq, step) - beq
            step_proj = np.copy(step)
            step_proj[~free_bd] = 0.0
            all_reduct = np.empty(n_samples)
            for i in range(n_samples):
                sin_value = 2.0 * t_samples[i] / (1.0 + t_samples[i] ** 2.0)
                step_alt = np.maximum(xl, np.minimum(step + sin_value * (sd - t_samples[i] * step_proj), xu))
                resid_ub_alt = np.maximum(np.dot(aub, step_alt) - bub, 0.0)
                resid_eq_alt = np.dot(aeq, step_alt) - beq
                all_reduct[i] = 0.5 * (np.inner(resid_ub, resid_ub) + np.inner(resid_eq, resid_eq) - np.inner(resid_ub_alt, resid_ub_alt) - np.inner(resid_eq_alt, resid_eq_alt))
            if np.all(all_reduct <= 0.0):
                # No reduction in the objective function is obtained.
                break

            # Accept the angle that provides the largest reduction in the
            # objective function, and update the iterate.
            # TODO: Can the gradient be updated smartly?
            i_max = np.argmax(all_reduct)
            cos_value = (1.0 - t_samples[i_max] ** 2.0) / (1.0 + t_samples[i_max] ** 2.0)
            sin_value = 2.0 * t_samples[i_max] / (1.0 + t_samples[i_max] ** 2.0)
            step[free_bd] = cos_value * step[free_bd] + sin_value * sd[free_bd]
            grad = np.dot(aub.T, np.maximum(np.dot(aub, step) - bub, 0.0)) + np.dot(aeq.T, (np.dot(aeq, step) - beq))
            reduct += all_reduct[i_max]

            # If the above angle is restricted by bound constraints, add them to
            # the working set, and restart the alternative iteration. Otherwise,
            # the calculations are terminated.
            if t_bd < 1.0 and i_max == n_samples - 1:
                if t_xl <= t_bd:
                    i_new = _argmin(all_t_xl)
                    step[i_new] = xl[i_new]
                    free_bd[i_new] = False
                if t_xu <= t_bd:
                    i_new = _argmin(all_t_xu)
                    step[i_new] = xu[i_new]
                    free_bd[i_new] = False
            else:
                break

    if debug:
        assert np.all(xl <= step)
        assert np.all(step <= xu)
        assert np.linalg.norm(step) < 1.1 * delta
    return step


def get_qr_tangential(aub, aeq, free_xl, free_xu, free_ub):
    n = free_xl.size
    identity = np.eye(n)
    q, r, _ = qr(np.r_[aeq, aub[~free_ub, :], -identity[~free_xl, :], identity[~free_xu, :]].T, pivoting=True)
    n_act = np.count_nonzero(np.abs(np.diag(r)) >= 10.0 * np.finfo(float).eps * n * np.linalg.norm(r[:np.min(r.shape), :np.min(r.shape)], axis=0))
    return n_act, q


def get_qr_normal(aub, free_xl, free_xu, free_slack, free_ub):
    m_linear_ub, n = aub.shape
    identity_n = np.eye(n)
    identity_m = np.eye(m_linear_ub)
    q, r, _ = qr(np.r_[
        np.c_[aub[~free_ub, :], -identity_m[~free_ub, :]],
        np.c_[np.zeros((m_linear_ub - np.count_nonzero(free_slack), n)), -identity_m[~free_slack, :]],
        np.c_[-identity_n[~free_xl, :], np.zeros((n - np.count_nonzero(free_xl), m_linear_ub))],
        np.c_[identity_n[~free_xu, :], np.zeros((n - np.count_nonzero(free_xu), m_linear_ub))],
    ].T, pivoting=True)
    n_act = np.count_nonzero(np.abs(np.diag(r)) >= 10.0 * np.finfo(float).eps * (n + m_linear_ub) * np.linalg.norm(r[:np.min(r.shape), :np.min(r.shape)], axis=0))
    return n_act, q


def _get_alpha_tr(step, sd, delta):
    step_sd = np.inner(step, sd)
    sd_sq = np.inner(sd, sd)
    dist_tr_sq = delta ** 2.0 - np.inner(step, step)
    temp = np.sqrt(step_sd ** 2.0 + sd_sq * dist_tr_sq)
    if step_sd <= 0.0 and sd_sq > np.finfo(float).tiny * abs(temp - step_sd):
        alpha_tr = max((temp - step_sd) / sd_sq, 0.0)
    elif abs(temp + step_sd) > np.finfo(float).tiny * dist_tr_sq:
        alpha_tr = max(dist_tr_sq / (temp + step_sd), 0.0)
    else:
        raise ZeroDivisionError
    return alpha_tr


def _argmax(x):
    return np.flatnonzero(x >= np.max(x))


def _argmin(x):
    return np.flatnonzero(x <= np.min(x))
